{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf5e1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic reporting functions for Bayesian Analysis:\n",
    "def report_posterior_stats(trace):\n",
    "    print(\"Posterior Statistics:\\n\")\n",
    "    summary = az.summary(trace, var_names=['mu', 'sigma'])\n",
    "    print(\"Posterior Summary:\\n\", summary) \n",
    "\n",
    "def report_histogram_stats(parameter_samples, random_distribution='normal', interested='Mu'):\n",
    "    print(\"Histogram Statistics:\\n\")\n",
    "    random_dist = {'normal': np.random.normal(0, 0.01, 1000),\n",
    "                   'gamma': np.random.gamma(2, 0.01, 1000),}\n",
    "    \n",
    "    # For Prior Param (from the 1000 samples used in histogram)\n",
    "    prior_param_samples = random_dist.get(random_distribution, np.random.normal(0, 0.01, 1000))\n",
    "    prior_param_hist, prior_param_bins = np.histogram(parameter_samples, bins=30)\n",
    "    prior_param_mode_bin = prior_param_bins[np.argmax(prior_param_hist)]\n",
    "    print(f\"Prior {interested} - Min:\", np.min(prior_param_samples), \"Max:\", np.max(prior_param_samples))\n",
    "    print(f\"Prior {interested} - Mode (approx bin center):\", prior_param_mode_bin, \"Count at mode:\", np.max(prior_param_hist))\n",
    "\n",
    "    # For Prior Param\n",
    "    post_param_hist, post_param_bins = np.histogram(parameter_samples, bins=30)\n",
    "    post_param_mode_bin = post_param_bins[np.argmax(post_param_hist)]\n",
    "    print(\"Posterior {interested} - Min:\", np.min(parameter_samples), \"Max:\", np.max(parameter_samples))\n",
    "    print(\"Posterior {interested} - Mode (approx bin center):\", post_param_mode_bin, \"Count at mode:\", np.max(post_param_hist))\n",
    "\n",
    "def report_MCMCtrace(mu_samples):\n",
    "    print(\"MCMC Trace Statistics:\\n\")\n",
    "    print(\"Mu Trace - Min:\", np.min(mu_samples), \"Max:\", np.max(mu_samples), \"Mean:\", np.mean(mu_samples))\n",
    "    print(\"Mu Trace - Autocorrelation at lag 1:\", pd.Series(mu_samples).autocorr(lag=1))  # Simple lag-1 autocorr; should be low for good mixing\n",
    "\n",
    "def report_predictive_stats(pred_mean, pred_ci):\n",
    "    print(\"Predictive Returns Statistics:\\n\")\n",
    "    # Overall stats\n",
    "    print(\"Predictive Returns - Overall Mean:\", np.mean(pred_mean))\n",
    "    print(\"Predictive Returns - Overall 95% CI Lower Mean:\", np.mean(pred_ci[0]), \"Upper Mean:\", np.mean(pred_ci[1]))\n",
    "\n",
    "    # Specific days for spot-check (e.g., day 1 and day 30; 0-indexed)\n",
    "    print(\"Predictive Returns Day 1 - Mean:\", pred_mean[0], \"95% CI:\", pred_ci[:, 0])\n",
    "    print(\"Predictive Returns Day 30 - Mean:\", pred_mean[29], \"95% CI:\", pred_ci[:, 29])\n",
    "\n",
    "def report_posterior_density(mu_kde_vals, sigma_kde_vals, mu_samples, sigma_samples):\n",
    "    print(\"Posterior Density Statistics:\\n\")\n",
    "    print(\"Posterior Mu KDE - Max Density Value:\", np.max(mu_kde_vals))\n",
    "    print(\"Posterior Mu KDE - Mode (approx):\", x_mu[np.argmax(mu_kde_vals)])\n",
    "    print(\"Posterior Mu KDE - Range:\", np.min(mu_samples), \"to\", np.max(mu_samples))\n",
    "    print(\"Posterior Sigma KDE - Max Density Value:\", np.max(sigma_kde_vals))\n",
    "    print(\"Posterior Sigma KDE - Mode (approx):\", x_sigma[np.argmax(sigma_kde_vals)])\n",
    "    print(\"Posterior Sigma KDE - Range:\", np.min(sigma_samples), \"to\", np.max(sigma_samples))\n",
    "\n",
    "def report_autocorr(mu_samples, sigma_samples):\n",
    "    from statsmodels.tsa.stattools import acf\n",
    "    print(\"Autocorrelation Analysis:\\n\")\n",
    "    mu_acf = acf(mu_samples, nlags=20, fft=False)\n",
    "    sigma_acf = acf(sigma_samples, nlags=20, fft=False)\n",
    "    print(\"Mu Autocorrelation (Lags 0-20):\", mu_acf)\n",
    "    print(\"Sigma Autocorrelation (Lags 0-20):\", sigma_acf)\n",
    "    # Check if autocorrelations drop below threshold (e.g., |0.1|)\n",
    "    print(\"Mu Autocorrelation - Lags where |ACF| < 0.1:\", np.where(np.abs(mu_acf) < 0.1)[0])\n",
    "    print(\"Sigma Autocorrelation - Lags where |ACF| < 0.1:\", np.where(np.abs(sigma_acf) < 0.1)[0])\n",
    "\n",
    "def report_posterior_predictive(observed_returns, simulated_returns):\n",
    "    from scipy.stats import ks_2samp\n",
    "    print(\"Posterior Predictive Check (PPC) Statistics:\\n\")\n",
    "    print(\"PPC - Observed Returns Mean:\", np.mean(observed_returns))\n",
    "    print(\"PPC - Observed Returns Std:\", np.std(observed_returns))\n",
    "    print(\"PPC - Simulated Returns Mean:\", np.mean(simulated_returns))\n",
    "    print(\"PPC - Simulated Returns Std:\", np.std(simulated_returns))\n",
    "    # Kolmogorov-Smirnov test for distributional similarity\n",
    "    ks_stat, ks_pval = ks_2samp(observed_returns, simulated_returns)\n",
    "    print(\"PPC - KS Test Statistic:\", ks_stat, \"P-value:\", ks_pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d0554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All-in-one plotting function: \n",
    "def save_fig_html(fig, filename:str, title:str, extension='html',path=None):\n",
    "    import os \n",
    "    if path and os.path.isdir(path):\n",
    "        filename = f\"{path}/{filename}\"\n",
    "    fig.update_layout(title=title, showlegend=True)\n",
    "    fig.write_html(f'{filename}.{extension}')  # Export for GitHub Pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361846f",
   "metadata": {},
   "source": [
    "Scenario #01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acf6f0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_686548/3989990771.py:6: FutureWarning:\n",
      "\n",
      "YF.download() has changed argument auto_adjust default to True\n",
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data\n",
    "df = yf.download('AAPL', start='2020-06-01', end='2025-06-30')  # New timeframe: June 2020–June 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7dd6148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-02</th>\n",
       "      <td>78.126457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-03</th>\n",
       "      <td>78.488136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-04</th>\n",
       "      <td>78.920197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-05</th>\n",
       "      <td>78.240532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-23</th>\n",
       "      <td>200.772141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-24</th>\n",
       "      <td>201.271576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-25</th>\n",
       "      <td>200.072937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-26</th>\n",
       "      <td>201.331512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-27</th>\n",
       "      <td>200.772141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1276 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker            AAPL\n",
       "Date                  \n",
       "2020-06-01         NaN\n",
       "2020-06-02   78.126457\n",
       "2020-06-03   78.488136\n",
       "2020-06-04   78.920197\n",
       "2020-06-05   78.240532\n",
       "...                ...\n",
       "2025-06-23  200.772141\n",
       "2025-06-24  201.271576\n",
       "2025-06-25  200.072937\n",
       "2025-06-26  201.331512\n",
       "2025-06-27  200.772141\n",
       "\n",
       "[1276 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Close.shift(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "303eb1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>LogReturn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>78.126457</td>\n",
       "      <td>78.247828</td>\n",
       "      <td>77.000132</td>\n",
       "      <td>77.131214</td>\n",
       "      <td>80791200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-02</th>\n",
       "      <td>78.488136</td>\n",
       "      <td>78.512412</td>\n",
       "      <td>77.417644</td>\n",
       "      <td>77.859436</td>\n",
       "      <td>87642800</td>\n",
       "      <td>0.004619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-03</th>\n",
       "      <td>78.920197</td>\n",
       "      <td>79.182362</td>\n",
       "      <td>78.235663</td>\n",
       "      <td>78.808537</td>\n",
       "      <td>104491200</td>\n",
       "      <td>0.005490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-04</th>\n",
       "      <td>78.240532</td>\n",
       "      <td>79.041577</td>\n",
       "      <td>77.866708</td>\n",
       "      <td>78.743009</td>\n",
       "      <td>87560400</td>\n",
       "      <td>-0.008649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-05</th>\n",
       "      <td>80.468887</td>\n",
       "      <td>80.529573</td>\n",
       "      <td>78.461416</td>\n",
       "      <td>78.490544</td>\n",
       "      <td>137250400</td>\n",
       "      <td>0.028083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-23</th>\n",
       "      <td>201.271576</td>\n",
       "      <td>202.070672</td>\n",
       "      <td>198.734462</td>\n",
       "      <td>201.401433</td>\n",
       "      <td>55814300</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-24</th>\n",
       "      <td>200.072937</td>\n",
       "      <td>203.209377</td>\n",
       "      <td>199.973044</td>\n",
       "      <td>202.360334</td>\n",
       "      <td>54064000</td>\n",
       "      <td>-0.005973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-25</th>\n",
       "      <td>201.331512</td>\n",
       "      <td>203.439121</td>\n",
       "      <td>200.392576</td>\n",
       "      <td>201.221637</td>\n",
       "      <td>39525700</td>\n",
       "      <td>0.006271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-26</th>\n",
       "      <td>200.772141</td>\n",
       "      <td>202.410281</td>\n",
       "      <td>199.233893</td>\n",
       "      <td>201.201646</td>\n",
       "      <td>50799100</td>\n",
       "      <td>-0.002782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-27</th>\n",
       "      <td>200.852051</td>\n",
       "      <td>202.989624</td>\n",
       "      <td>199.773273</td>\n",
       "      <td>201.661130</td>\n",
       "      <td>73188600</td>\n",
       "      <td>0.000398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1276 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price            Close        High         Low        Open     Volume  \\\n",
       "Ticker            AAPL        AAPL        AAPL        AAPL       AAPL   \n",
       "Date                                                                    \n",
       "2020-06-01   78.126457   78.247828   77.000132   77.131214   80791200   \n",
       "2020-06-02   78.488136   78.512412   77.417644   77.859436   87642800   \n",
       "2020-06-03   78.920197   79.182362   78.235663   78.808537  104491200   \n",
       "2020-06-04   78.240532   79.041577   77.866708   78.743009   87560400   \n",
       "2020-06-05   80.468887   80.529573   78.461416   78.490544  137250400   \n",
       "...                ...         ...         ...         ...        ...   \n",
       "2025-06-23  201.271576  202.070672  198.734462  201.401433   55814300   \n",
       "2025-06-24  200.072937  203.209377  199.973044  202.360334   54064000   \n",
       "2025-06-25  201.331512  203.439121  200.392576  201.221637   39525700   \n",
       "2025-06-26  200.772141  202.410281  199.233893  201.201646   50799100   \n",
       "2025-06-27  200.852051  202.989624  199.773273  201.661130   73188600   \n",
       "\n",
       "Price      LogReturn  \n",
       "Ticker                \n",
       "Date                  \n",
       "2020-06-01       NaN  \n",
       "2020-06-02  0.004619  \n",
       "2020-06-03  0.005490  \n",
       "2020-06-04 -0.008649  \n",
       "2020-06-05  0.028083  \n",
       "...              ...  \n",
       "2025-06-23  0.002484  \n",
       "2025-06-24 -0.005973  \n",
       "2025-06-25  0.006271  \n",
       "2025-06-26 -0.002782  \n",
       "2025-06-27  0.000398  \n",
       "\n",
       "[1276 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LogReturn'] = np.log(df['Close'].values / df['Close'].shift(1).values)\n",
    "returns = df['LogReturn'].dropna().values  # ~1250 daily returns\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fc2482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_686548/1244083656.py:10: FutureWarning:\n",
      "\n",
      "YF.download() has changed argument auto_adjust default to True\n",
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu, sigma, pred_returns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6cab27350e4dd7a318af747e1ffe20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n",
      "Sampling: [pred_returns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efa9ef77e5140c9a918687ac33b7d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior Statistics:\n",
      "\n",
      "Posterior Summary:\n",
      "         mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\n",
      "mu     0.001  0.001  -0.000    0.002        0.0      0.0   10519.0    3245.0   \n",
      "sigma  0.018  0.000   0.017    0.019        0.0      0.0    9055.0    2557.0   \n",
      "\n",
      "       r_hat  \n",
      "mu       1.0  \n",
      "sigma    1.0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram Statistics:\n",
      "\n",
      "Prior Mu - Min: -0.026156089614560993 Max: 0.03174871547173786\n",
      "Prior Mu - Mode (approx bin center): 0.0007245348567461118 Count at mode: 410\n",
      "Posterior {interested} - Min: -0.0012386010469329954 Max: 0.002442278772465331\n",
      "Posterior {interested} - Mode (approx bin center): 0.0007245348567461118 Count at mode: 410\n",
      "Histogram Statistics:\n",
      "\n",
      "Prior Sigma - Min: 0.0006177196842074164 Max: 0.09871193953086606\n",
      "Prior Sigma - Mode (approx bin center): 0.01806057473078925 Count at mode: 423\n",
      "Posterior {interested} - Min: 0.01664015871321798 Max: 0.019480990748360523\n",
      "Posterior {interested} - Mode (approx bin center): 0.01806057473078925 Count at mode: 423\n",
      "MCMC Trace Statistics:\n",
      "\n",
      "Mu Trace - Min: -0.0012386010469329954 Max: 0.002442278772465331 Mean: 0.0006546970527129747\n",
      "Mu Trace - Autocorrelation at lag 1: -0.42950909085500644\n",
      "Predictive Returns Statistics:\n",
      "\n",
      "Predictive Returns - Overall Mean: 0.0006700159162032692\n",
      "Predictive Returns - Overall 95% CI Lower Mean: -0.03469554027875853 Upper Mean: 0.0361984073566074\n",
      "Predictive Returns Day 1 - Mean: 0.00011218712082693882 95% CI: [-0.0351968   0.03618777]\n",
      "Predictive Returns Day 30 - Mean: 0.0004947351868591649 95% CI: [-0.03512264  0.03613885]\n",
      "Autocorrelation Analysis:\n",
      "\n",
      "Mu Autocorrelation (Lags 0-20): [ 1.         -0.42943488  0.17349436 -0.06217533  0.02735717 -0.00872041\n",
      " -0.01402789 -0.00404192  0.00984499 -0.01158694 -0.01055201  0.02570824\n",
      " -0.01378077  0.00191548  0.00170767  0.00638661  0.01399287 -0.02489682\n",
      "  0.0352724  -0.05456727  0.02678697]\n",
      "Sigma Autocorrelation (Lags 0-20): [ 1.         -0.39275804  0.16985218 -0.07233414  0.03039785 -0.00946345\n",
      "  0.00290773 -0.0036274  -0.00368892 -0.00135671 -0.00371937 -0.01516248\n",
      "  0.00730404  0.01213804 -0.00562193  0.02737258  0.00142733 -0.02418\n",
      "  0.03873412 -0.02460694  0.02652084]\n",
      "Mu Autocorrelation - Lags where |ACF| < 0.1: [ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Sigma Autocorrelation - Lags where |ACF| < 0.1: [ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "Posterior Density Statistics:\n",
      "\n",
      "Posterior Mu KDE - Max Density Value: 771.0776719717499\n",
      "Posterior Mu KDE - Mode (approx): 0.000731970977593381\n",
      "Posterior Mu KDE - Range: -0.0012386010469329954 to 0.002442278772465331\n",
      "Posterior Sigma KDE - Max Density Value: 1099.1271944588493\n",
      "Posterior Sigma KDE - Mode (approx): 0.01813231291349487\n",
      "Posterior Sigma KDE - Range: 0.01664015871321798 to 0.019480990748360523\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 1. Load Data\n",
    "df = yf.download('AAPL', start='2020-09-21', end='2025-09-21')\n",
    "trading_freq = 1 # Daily trading: 1, Weekly: 5, Monthly: 21\n",
    "df['LogReturn'] = np.log(df['Close'].values / df['Close'].shift(trading_freq).values)\n",
    "returns = df['LogReturn'].dropna().values  # ~1,260 daily returns\n",
    "\n",
    "# 2. Bayesian Volatility Model\n",
    "with pm.Model() as model:\n",
    "    # Priors: Normal-Inverse-Gamma for mean (mu) and variance (sigma2)\n",
    "    mu = pm.Normal('mu', mu=0, sigma=0.01)\n",
    "    sigma = pm.InverseGamma('sigma', alpha=2, beta=0.1)\n",
    "    # Likelihood\n",
    "    returns_obs = pm.Normal('returns_obs', mu=mu, sigma=sigma, observed=returns)\n",
    "    # Predictive variable for future returns\n",
    "    pred_returns = pm.Normal('pred_returns', mu=mu, sigma=sigma, shape=30)  # 30-day forecast\n",
    "    # Sample posterior\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True)\n",
    "    report_posterior_stats(trace) \n",
    "\n",
    "# 3. Posterior Predictive Sampling\n",
    "with model:\n",
    "    pred_trace = pm.sample_posterior_predictive(trace, var_names=['pred_returns'])\n",
    "\n",
    "# 4. Extract Posterior and Predictive Samples\n",
    "posterior = az.extract(trace)\n",
    "mu_samples = posterior['mu'].values  # Shape: (4000,) after flattening chains\n",
    "\n",
    "sigma_samples = posterior['sigma'].values  # Shape: (4000,)\n",
    "pred_samples = pred_trace.posterior_predictive['pred_returns'].values  # Shape: (chains, draws, 30)\n",
    "\n",
    "# Save trace for reproducibility\n",
    "az.to_netcdf(trace, 'trace.nc')\n",
    "az.to_netcdf(pred_trace, 'pred_trace.nc')\n",
    "\n",
    "# 5. Interactive Visualization\n",
    "#subplot_titles = ['Prior vs Posterior: Mu', \n",
    "#                  'Prior vs Posterior: Sigma', \n",
    "#                  'MCMC Trace: Mu', \n",
    "#                  'Predictive Returns (30 Days)', \n",
    "#                  'Posterior Density Plot',\n",
    "#                  'Autocorrelations']\n",
    "#fig = make_subplots(rows=1, cols=1, subplot_titles=subplot_titles)\n",
    "\n",
    "# Prior vs Posterior: Mu\n",
    "row, col= 1, 1\n",
    "fig = make_subplots(rows=row, cols=col)\n",
    "fig.add_trace(go.Histogram(x=np.random.normal(0, 0.01, 1000), name='Prior Mu', opacity=0.5), row=row, col=col)\n",
    "fig.add_trace(go.Histogram(x=mu_samples, name='Posterior Mu', opacity=0.5), row=row, col=col)\n",
    "## Add Posterior Predictive returns histogram\n",
    "fig.add_trace(go.Histogram(x=returns, name='Observed Returns', opacity=0.5, nbinsx=50), row=row, col=col)  # Adjust row/col as needed\n",
    "simulated_returns = pred_trace.posterior_predictive['pred_returns'].values.flatten()[:1000]  # Subsample for clarity\n",
    "fig.add_trace(go.Histogram(x=simulated_returns, name='Simulated Returns', opacity=0.5, nbinsx=50), row=row, col=col)\n",
    "save_fig_html(fig, 's1_prior_post_mu', 'Prior vs Posterior: Average(Mu) Daily Return')\n",
    "report_histogram_stats(mu_samples, random_distribution='normal', interested='Mu')\n",
    "\n",
    "# Prior vs Posterior: Sigma\n",
    "fig = make_subplots(rows=row, cols=col)\n",
    "fig.add_trace(go.Histogram(x=np.random.gamma(2, 0.1, 1000), name='Prior Sigma', opacity=0.5), row=row, col=col)\n",
    "fig.add_trace(go.Histogram(x=sigma_samples, name='Posterior Sigma', opacity=0.5), row=row, col=col)\n",
    "save_fig_html(fig, 's1_prior_post_sigma', 'Prior vs Posterior: Volatility(Sigma) of Daily Return', path='../plots')\n",
    "report_histogram_stats(sigma_samples, random_distribution='gamma', interested='Sigma')\n",
    "\n",
    "# MCMC Trace: Mu\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(mu_samples)), y=mu_samples, mode='lines', name='Mu Trace'), row=row, col=col)\n",
    "save_fig_html(fig, 's1_mcmc_trace_mu', 'MCMC Trace: Mu', path='../plots') \n",
    "report_MCMCtrace(mu_samples)\n",
    "\n",
    "# Predictive Returns: Mean and 95% CI\n",
    "fig = make_subplots(rows=1, cols=1) \n",
    "pred_mean = pred_samples.mean(axis=(0, 1))  # Mean over chains and draws\n",
    "pred_ci = np.percentile(pred_samples, [2.5, 97.5], axis=(0, 1))  # 95% credible interval\n",
    "report_predictive_stats(pred_mean, pred_ci)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(30), y=pred_mean, mode='lines', name='Mean Pred Returns'), row=row, col=col)\n",
    "fig.add_trace(go.Scatter(x=np.arange(30), y=pred_ci[0], mode='lines', name='95% CI Lower', line=dict(dash='dash')), row=row, col=col)\n",
    "fig.add_trace(go.Scatter(x=np.arange(30), y=pred_ci[1], mode='lines', name='95% CI Upper', line=dict(dash='dash')), row=row, col=col)\n",
    "save_fig_html(fig, 's1_predictive_returns', 'Predictive Returns (30 Days)', path='../plots')\n",
    "\n",
    "# Autocorrelations\n",
    "from statsmodels.tsa.stattools import acf\n",
    "# Autocorrelation\n",
    "mu_acf = acf(mu_samples, nlags=20, fft=False)\n",
    "sigma_acf = acf(sigma_samples, nlags=20, fft=False)\n",
    "fig = make_subplots(rows=1, cols=1) \n",
    "fig.add_trace(go.Bar(x=np.arange(21), y=mu_acf, name='Mu Autocorrelation'), row=row, col=col)  # Adjust row/col\n",
    "fig.add_trace(go.Bar(x=np.arange(21), y=sigma_acf, name='Sigma Autocorrelation'), row=row, col=col)\n",
    "#fig.update_xaxes(title_text=\"Lag\", row=1, col=2)\n",
    "#fig.update_yaxes(title_text=\"Autocorrelation\", row=1, col=2)\n",
    "save_fig_html(fig, 's1_autocorrelations', 'Autocorrelations (Mu and Sigma)', path='../plots')\n",
    "report_autocorr(mu_samples, sigma_samples) \n",
    "\n",
    "\n",
    "# Posterior Density Plot: KDE for Mu and Sigma\n",
    "import scipy.stats as stats\n",
    "fig = make_subplots(rows=1, cols=1) \n",
    "mu_kde = stats.gaussian_kde(mu_samples)\n",
    "sigma_kde = stats.gaussian_kde(sigma_samples)\n",
    "x_mu = np.linspace(np.min(mu_samples), np.max(mu_samples), 100)\n",
    "x_sigma = np.linspace(np.min(sigma_samples), np.max(sigma_samples), 100)\n",
    "mu_kde_vals = mu_kde(x_mu) # Evaluate KDE on grid \n",
    "sigma_kde_vals = sigma_kde(x_sigma) # Evaluate KDE on grid \n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_mu, y=mu_kde(x_mu), mode='lines', name='Posterior Mu KDE'), row=row, col=col)  # Adjust row/col\n",
    "fig.add_trace(go.Scatter(x=x_sigma, y=sigma_kde(x_sigma), mode='lines', name='Posterior Sigma KDE'), row=row, col=col)\n",
    "# Add HDI lines (from az.summary: mu [-0.000, 0.002], sigma [0.017, 0.019])\n",
    "fig.add_vline(x=-0.000, line_dash=\"dash\", line_color=\"blue\", row=row, col=col)\n",
    "fig.add_vline(x=0.002, line_dash=\"dash\", line_color=\"blue\", row=row, col=col)\n",
    "fig.add_vline(x=0.017, line_dash=\"dash\", line_color=\"orange\", row=row, col=col)\n",
    "fig.add_vline(x=0.019, line_dash=\"dash\", line_color=\"orange\", row=row, col=col)\n",
    "#fig.update_xaxes(title_text=\"Parameter Value\", row=2, col=2)\n",
    "#fig.update_yaxes(title_text=\"Density\", row=2, col=2)\n",
    "save_fig_html(fig, 's1_post_density', 'Posterior Density (Mu and Sigma)', path='../plots')\n",
    "report_posterior_density(mu_kde_vals, sigma_kde_vals, mu_samples, sigma_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bcdd0",
   "metadata": {},
   "source": [
    "---\n",
    "Scenario #02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu_alpha, sigma_alpha, mu_beta, sigma_beta, alpha, beta, pred_times]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d247151ae34f9b9450cc4bca2797c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 223 seconds.\n",
      "/tmp/ipykernel_29394/3570969171.py:73: UserWarning: The effect of Potentials on other parameters is ignored during posterior predictive sampling. This is likely to lead to invalid or biased predictive samples.\n",
      "  pred_trace = pm.sample_posterior_predictive(trace, var_names=['pred_times'])\n",
      "Sampling: [pred_times]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28814018e09e45f18cab4541f084765d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 1. Load and Preprocess Data (Updated URL)\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "online_retail = fetch_ucirepo(id=352)  # UCI ID for Online Retail\n",
    "df = online_retail.data.original  # Raw dataframe\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# Compute time-to-next-purchase\n",
    "df = df.sort_values(['CustomerID', 'InvoiceDate'])\n",
    "df['NextPurchase'] = df.groupby('CustomerID')['InvoiceDate'].shift(-1)\n",
    "df['TimeToNext'] = (df['NextPurchase'] - df['InvoiceDate']).dt.days\n",
    "df['Censored'] = df['TimeToNext'].isna().astype(int)  # 1 if no next purchase (censored)\n",
    "df['TimeToNext'] = df['TimeToNext'].fillna(365)  # Censor at 1 year\n",
    "df['TimeToNext'] = df['TimeToNext'].clip(lower=0.1)  # Ensure times > 0 (add 0.1 for same-day purchases)\n",
    "\n",
    "# Aggregate by customer, select top countries\n",
    "df['Country'] = df['Country'].replace(['EIRE', 'Channel Islands'], 'Other')\n",
    "top_countries = df['Country'].value_counts().head(5).index\n",
    "df = df[df['Country'].isin(top_countries)]\n",
    "customer_data = df.groupby(['CustomerID', 'Country']).agg({\n",
    "    'TimeToNext': 'min',\n",
    "    'Censored': 'min'\n",
    "}).reset_index()\n",
    "\n",
    "# Encode countries and validate shapes\n",
    "customer_data = customer_data.reset_index(drop=True)\n",
    "country_idx = pd.Categorical(customer_data['Country']).codes\n",
    "times = customer_data['TimeToNext'].values\n",
    "censored = customer_data['Censored'].values\n",
    "n_customers = len(customer_data)\n",
    "assert len(country_idx) == len(times) == len(censored) == n_customers, \"Shape mismatch in data arrays\"\n",
    "assert all(times > 0), f\"Invalid times: {times[times <= 0]}\"  # Ensure all times positive\n",
    "\n",
    "# 2. Bayesian Weibull Survival Model\n",
    "with pm.Model() as survival_model:\n",
    "    # Hyperpriors for country-level parameters\n",
    "    mu_alpha = pm.HalfNormal('mu_alpha', sigma=2)  # Positive prior\n",
    "    sigma_alpha = pm.HalfNormal('sigma_alpha', sigma=1)\n",
    "    mu_beta = pm.HalfNormal('mu_beta', sigma=2)    # Positive prior\n",
    "    sigma_beta = pm.HalfNormal('sigma_beta', sigma=1)\n",
    "    \n",
    "    # Country-specific parameters (positive)\n",
    "    n_countries = len(np.unique(country_idx))\n",
    "    alpha = pm.HalfNormal('alpha', sigma=sigma_alpha, shape=n_countries)\n",
    "    beta = pm.HalfNormal('beta', sigma=sigma_beta, shape=n_countries)\n",
    "    \n",
    "    # Map parameters to customers\n",
    "    alpha_i = alpha[country_idx]\n",
    "    beta_i = beta[country_idx]\n",
    "    \n",
    "    # Weibull likelihood for uncensored data\n",
    "    pm.Weibull('t_uncensored', alpha=alpha_i[censored == 0], beta=beta_i[censored == 0], \n",
    "               observed=times[censored == 0])\n",
    "    \n",
    "    # Log-survival for censored data\n",
    "    censored_logsurv = -((times[censored == 1] / beta_i[censored == 1]) ** alpha_i[censored == 1])\n",
    "    pm.Potential('censored_logsurv', censored_logsurv.sum())\n",
    "    \n",
    "    # Predictive survival times\n",
    "    pred_times = pm.Weibull('pred_times', alpha=alpha_i, beta=beta_i, shape=n_customers)\n",
    "    \n",
    "    # Sample posterior\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True, target_accept=0.9)\n",
    "\n",
    "# 3. Posterior Predictive Sampling\n",
    "with survival_model:\n",
    "    pred_trace = pm.sample_posterior_predictive(trace, var_names=['pred_times'])\n",
    "\n",
    "# 4. Extract Posterior and Predictive Samples\n",
    "posterior = az.extract(trace)\n",
    "alpha_samples = posterior['alpha'].values  # Shape: (chains*draws, n_countries)\n",
    "beta_samples = posterior['beta'].values    # Shape: (chains*draws, n_countries)\n",
    "pred_times_samples = pred_trace.posterior_predictive['pred_times'].values  # Shape: (chains, draws, n_customers)\n",
    "\n",
    "# Save traces\n",
    "az.to_netcdf(trace, 'survival_trace.nc')\n",
    "az.to_netcdf(pred_trace, 'survival_pred_trace.nc')\n",
    "\n",
    "# 5. Interactive Visualization\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Posterior Alpha by Country', 'Survival Curves by Country'))\n",
    "\n",
    "# Posterior Alpha\n",
    "countries = pd.Categorical(customer_data['Country']).categories\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "for i, country in enumerate(countries):\n",
    "    fig.add_trace(go.Histogram(x=alpha_samples[:, i], name=f'Alpha: {country}', \n",
    "                               marker_color=colors[i], opacity=0.6), rows=row, cols=col)\n",
    "\n",
    "# Survival Curves with Slider\n",
    "t = np.linspace(0.1, 365, 100)  # Start at 0.1 to avoid log(0)\n",
    "traces = []\n",
    "for i, country in enumerate(countries):\n",
    "    alpha_mean = alpha_samples[:, i].mean()\n",
    "    beta_mean = beta_samples[:, i].mean()\n",
    "    survival = np.exp(-((t / beta_mean) ** alpha_mean))\n",
    "    trace = go.Scatter(x=t, y=survival, mode='lines', name=f'Survival: {country}',\n",
    "                       line=dict(color=colors[i]), visible=(i == 0))\n",
    "    traces.append(trace)\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "# Slider for country selection\n",
    "steps = [\n",
    "    {'method': 'restyle', 'label': country, 'args': [{'visible': [j == i for j in range(len(countries))]}]}\n",
    "    for i, country in enumerate(countries)\n",
    "]\n",
    "fig.update_layout(\n",
    "    title='Bayesian Survival Analysis for Customer Lifetime Value',\n",
    "    xaxis2_title='Days to Next Purchase', yaxis2_title='Survival Probability',\n",
    "    sliders=[{'steps': steps, 'active': 0, 'currentvalue': {'prefix': 'Country: '}}],\n",
    "    showlegend=True\n",
    ")\n",
    "fig.write_html('survival_viz.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55784d",
   "metadata": {},
   "source": [
    "---\n",
    "Scenario #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e119c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.12/site-packages/pytensor/link/c/cmodule.py:2968: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.\n",
      "This usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.\n",
      "Alternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == 'NUMBA'` or passing `mode='NUMBA'` when compiling a PyTensor function.\n",
      "For more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library\n",
      "  warnings.warn(\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">NUTS: [beta, intercept]\n",
      ">BinaryGibbsMetropolis: [churn_prob]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8483730067da443a90dc57c922958f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2522 seconds.\n",
      "/root/anaconda3/lib/python3.12/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n",
      "Sampling: [churn_prob]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02d70a4e317451180ae8f58632f3327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load and Preprocess Data\n",
    "url = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\n",
    "df = pd.read_csv(url)\n",
    "# Handle missing TotalCharges (replace empty strings with NaN, then impute)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())\n",
    "# Select features: tenure, MonthlyCharges, Contract (categorical)\n",
    "df['Contract'] = df['Contract'].map({'Month-to-month': 0, 'One year': 1, 'Two year': 2})\n",
    "df = df[['tenure', 'MonthlyCharges', 'Contract', 'Churn']].dropna()\n",
    "# Encode Churn (Yes=1, No=0)\n",
    "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "df[['tenure', 'MonthlyCharges']] = scaler.fit_transform(df[['tenure', 'MonthlyCharges']])\n",
    "# Prepare data arrays\n",
    "X = df[['tenure', 'MonthlyCharges', 'Contract']].values\n",
    "y = df['Churn'].values\n",
    "n_samples, n_features = X.shape\n",
    "assert len(y) == n_samples, \"Mismatch in X and y shapes\"\n",
    "\n",
    "# 2. Bayesian Logistic Regression Model\n",
    "with pm.Model() as churn_model:\n",
    "    # Priors for coefficients\n",
    "    beta = pm.Normal('beta', mu=0, sigma=2, shape=n_features)\n",
    "    intercept = pm.Normal('intercept', mu=0, sigma=2)\n",
    "    # Linear combination\n",
    "    logits = pm.math.dot(X, beta) + intercept\n",
    "    # Likelihood\n",
    "    pm.Bernoulli('churn', logit_p=logits, observed=y)\n",
    "    # Posterior predictive for probabilities\n",
    "    churn_prob = pm.Bernoulli('churn_prob', logit_p=logits, shape=n_samples)\n",
    "    # Sample posterior\n",
    "    trace = pm.sample(1000, tune=1000, return_inferencedata=True, target_accept=0.9)\n",
    "\n",
    "# 3. Posterior Predictive Sampling\n",
    "with churn_model:\n",
    "    pred_trace = pm.sample_posterior_predictive(trace, var_names=['churn_prob'])\n",
    "\n",
    "# 4. Extract Posterior and Predictive Samples\n",
    "posterior = az.extract(trace)\n",
    "beta_samples = posterior['beta'].values  # Shape: (chains*draws, n_features)\n",
    "pred_probs = pred_trace.posterior_predictive['churn_prob'].mean(axis=(0, 1))  # Mean probability per customer\n",
    "\n",
    "# Save traces\n",
    "az.to_netcdf(trace, 'churn_trace.nc')\n",
    "az.to_netcdf(pred_trace, 'churn_pred_trace.nc')\n",
    "\n",
    "# 5. Interactive Visualization (ROC Curve with Threshold Slider)\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y, pred_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Posterior Beta Coefficients', 'ROC Curve with Threshold Slider'))\n",
    "\n",
    "# Posterior Beta Coefficients\n",
    "feature_names = ['tenure', 'MonthlyCharges', 'Contract']\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, name in enumerate(feature_names):\n",
    "    fig.add_trace(go.Histogram(x=beta_samples[:, i], name=f'Beta: {name}', \n",
    "                               marker_color=colors[i], opacity=0.6), rows=row, cols=col)\n",
    "\n",
    "# ROC Curve\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC Curve (AUC = {roc_auc:.2f})',\n",
    "                         line=dict(color='blue')), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random Guess',\n",
    "                         line=dict(color='black', dash='dash')), row=1, col=2)\n",
    "\n",
    "# Add classification point for threshold\n",
    "thresholds_subset = np.linspace(0, 1, 11)  # 0.0 to 1.0 in steps of 0.1\n",
    "traces = []\n",
    "for thresh in thresholds_subset:\n",
    "    idx = np.argmin(np.abs(thresholds - thresh))\n",
    "    trace = go.Scatter(x=[fpr[idx]], y=[tpr[idx]], mode='markers', name=f'Threshold: {thresh:.2f}',\n",
    "                       marker=dict(size=10, color='red'), visible=(thresh == 0.5))\n",
    "    traces.append(trace)\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "# Slider for classification threshold\n",
    "steps = [\n",
    "    {'method': 'restyle', 'label': f'{thresh:.2f}', \n",
    "     'args': [{'visible': [True, True] + [j == i for j in range(len(thresholds_subset))]}]}\n",
    "    for i, thresh in enumerate(thresholds_subset)\n",
    "]\n",
    "fig.update_layout(\n",
    "    title='Bayesian Churn Prediction for Telco Customers',\n",
    "    xaxis2_title='False Positive Rate', yaxis2_title='True Positive Rate',\n",
    "    sliders=[{'steps': steps, 'active': 5, 'currentvalue': {'prefix': 'Threshold: '}}],\n",
    "    showlegend=True\n",
    ")\n",
    "fig.write_html('churn_viz.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dcc7ea",
   "metadata": {},
   "source": [
    "---\n",
    "Scenario #4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a23f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "/root/anaconda3/lib/python3.12/site-packages/pytensor/link/c/cmodule.py:2968: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.\n",
      "This usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.\n",
      "Alternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == 'NUMBA'` or passing `mode='NUMBA'` when compiling a PyTensor function.\n",
      "For more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library\n",
      "  warnings.warn(\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [user_prefs, song_features]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3ba150e01743d993c079c11cfc7ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 18211 seconds.\n",
      "There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "Chain 0 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "Chain 1 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "Chain 2 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "Chain 3 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Sampling: [observed_counts]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781a42938369439c92d2762c328332d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     75\u001b[0m user_prefs_mean \u001b[38;5;241m=\u001b[39m user_prefs_samples\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Shape: (n_users, n_latent)\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m user_tsne \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(user_prefs_mean)  \u001b[38;5;66;03m# Shape: (n_users, 2)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m fig \u001b[38;5;241m=\u001b[39m make_subplots(rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, subplot_titles\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser Preference Posterior\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt-SNE User Embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Posterior for one user’s latent dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:1175\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter\n\u001b[0;32m-> 1175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1176\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/manifold/_t_sne.py:864\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 864\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "# Since the above cell take too much time to run, let's run simpler version:\n",
    "# - size down: n_users, n_songs = 500, 500 \n",
    "# - Instead of pm.Poisson likelihood, use pm.Normal likelihood on log transformed counts.\n",
    "# - Fewer latent dimensions: n_latent = 5 \n",
    "# - Reduced tuning steps: tune=500 \n",
    "# - Sparse matrix: csr_matrix for preprocessing \n",
    "# - Numerical stability: 1e-8 to norms and clipped sim_scores to [-1, 1]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.manifold import TSNE\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pytensor.tensor as pt\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# 1. Load and Preprocess Data (Download and Unzip)\n",
    "url = \"http://labrosa.ee.columbia.edu/~dpwe/tmp/train_triplets.txt.zip\"\n",
    "response = requests.get(url)\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    with z.open('train_triplets.txt') as f:\n",
    "        df = pd.read_csv(f, sep='\\t', names=['user', 'song', 'count'])\n",
    "\n",
    "# Subsample for efficiency\n",
    "n_users, n_songs = 500, 500  # Reduced from 1000\n",
    "top_users = df['user'].value_counts().head(n_users).index\n",
    "top_songs = df['song'].value_counts().head(n_songs).index\n",
    "df = df[df['user'].isin(top_users) & df['song'].isin(top_songs)]\n",
    "# Create sparse user-song matrix\n",
    "play_counts = df.pivot_table(index='user', columns='song', values='count', fill_value=0)\n",
    "play_counts = csr_matrix(play_counts.values)  # Sparse matrix\n",
    "user_ids = pd.Categorical(df['user'].unique()).categories[:n_users]\n",
    "song_ids = pd.Categorical(df['song'].unique()).categories[:n_songs]\n",
    "n_samples = play_counts.shape[0]\n",
    "assert play_counts.shape == (n_users, n_songs), \"Play count matrix shape mismatch\"\n",
    "\n",
    "# Transform counts to stabilize variance (log1p for sparse data)\n",
    "play_counts_dense = np.log1p(play_counts.toarray())  # Log-transform for Normal likelihood\n",
    "\n",
    "# 2. Bayesian ABC Model\n",
    "n_latent = 5  # Reduced latent dimensions for speed\n",
    "with pm.Model() as abc_model:\n",
    "    # Priors for latent user preferences and song features\n",
    "    user_prefs = pm.Normal('user_prefs', mu=0, sigma=1, shape=(n_users, n_latent))\n",
    "    song_features = pm.Normal('song_features', mu=0, sigma=1, shape=(n_songs, n_latent))\n",
    "    \n",
    "    # Compute cosine similarity (vectorized)\n",
    "    dot_product = pm.math.sum(user_prefs[:, None, :] * song_features[None, :, :], axis=2)\n",
    "    user_norm = pm.math.sqrt(pm.math.sum(user_prefs**2, axis=1))\n",
    "    song_norm = pm.math.sqrt(pm.math.sum(song_features**2, axis=1))\n",
    "    sim_scores = dot_product / (user_norm[:, None] * song_norm[None, :] + 1e-8)  # Avoid division by zero\n",
    "    sim_scores = pm.math.clip(sim_scores, -1, 1)\n",
    "    \n",
    "    # Expected counts (Normal likelihood for log-transformed counts)\n",
    "    mu = 3 * sim_scores  # Scale similarity to match log-count scale\n",
    "    pm.Normal('observed_counts', mu=mu, sigma=1, observed=play_counts_dense)\n",
    "    \n",
    "    # Sample using ABC\n",
    "    trace = pm.sample(1000, tune=500, return_inferencedata=True, target_accept=0.95)\n",
    "\n",
    "# 3. Posterior Predictive Sampling\n",
    "with abc_model:\n",
    "    pred_trace = pm.sample_posterior_predictive(trace, var_names=['observed_counts'])\n",
    "\n",
    "# 4. Extract Posterior and Predictive Samples\n",
    "posterior = az.extract(trace)\n",
    "user_prefs_samples = posterior['user_prefs'].values  # Shape: (chains*draws, n_users, n_latent)\n",
    "song_features_samples = posterior['song_features'].values  # Shape: (chains*draws, n_songs, n_latent)\n",
    "pred_counts = np.expm1(pred_trace.posterior_predictive['observed_counts'].mean(axis=(0, 1)))  # Inverse log1p\n",
    "\n",
    "# Save traces\n",
    "az.to_netcdf(trace, 'music_trace.nc')\n",
    "az.to_netcdf(pred_trace, 'music_pred_trace.nc')\n",
    "\n",
    "# 5. Interactive Visualization (t-SNE with ABC Animation)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "user_prefs_mean = user_prefs_samples.mean(axis=0)  # Shape: (n_users, n_latent)\n",
    "user_tsne = tsne.fit_transform(user_prefs_mean)  # Shape: (n_users, 2)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('User Preference Posterior', 't-SNE User Embeddings'))\n",
    "\n",
    "# Posterior for one user’s latent dimension\n",
    "fig.add_trace(go.Histogram(x=user_prefs_samples[:, 0, 0], name='User 0: Latent Dim 1', \n",
    "                           marker_color='blue', opacity=0.6), rows=row, cols=col)\n",
    "\n",
    "# t-SNE Scatter\n",
    "fig.add_trace(go.Scatter(x=user_tsne[:, 0], y=user_tsne[:, 1], mode='markers',\n",
    "                         marker=dict(size=5, color='blue'), name='Users'), row=1, col=2)\n",
    "\n",
    "# Animation for posterior draws (simplified to 5 frames)\n",
    "n_frames = 5\n",
    "draws = np.linspace(0, user_prefs_samples.shape[0] - 1, n_frames, dtype=int)\n",
    "frames = []\n",
    "for i, draw in enumerate(draws):\n",
    "    tsne_frame = TSNE(n_components=2, random_state=42).fit_transform(user_prefs_samples[draw])\n",
    "    frame = go.Frame(\n",
    "        data=[go.Scatter(x=tsne_frame[:, 0], y=tsne_frame[:, 1], mode='markers',\n",
    "                         marker=dict(size=5, color='blue'), name='Users')],\n",
    "        name=f'Draw {i}'\n",
    "    )\n",
    "    frames.append(frame)\n",
    "\n",
    "fig.update(frames=frames)\n",
    "fig.update_layout(\n",
    "    title='Bayesian ABC for Music Recommendations',\n",
    "    xaxis2_title='t-SNE Dim 1', yaxis2_title='t-SNE Dim 2',\n",
    "    updatemenus=[{\n",
    "        'buttons': [\n",
    "            {'method': 'animate', 'label': 'Play', 'args': [None, {'frame': {'duration': 500, 'redraw': True}, 'fromcurrent': True}]},\n",
    "            {'method': 'animate', 'label': 'Pause', 'args': [[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate'}]}\n",
    "        ],\n",
    "        'direction': 'left', 'pad': {'r': 10, 't': 87}, 'showactive': True, 'type': 'buttons'\n",
    "    }],\n",
    "    showlegend=True\n",
    ")\n",
    "fig.write_html('music_viz.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698215c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading and unzipping dataset...\n",
      "INFO:pymc:Loading and unzipping dataset...\n",
      "Data loaded in 119.32 seconds\n",
      "INFO:pymc:Data loaded in 119.32 seconds\n",
      "Subsampling 50 users and 50 songs...\n",
      "INFO:pymc:Subsampling 50 users and 50 songs...\n",
      "Play counts matrix shape: (50, 50)\n",
      "INFO:pymc:Play counts matrix shape: (50, 50)\n",
      "Starting SMC sampling...\n",
      "INFO:pymc:Starting SMC sampling...\n",
      "Initializing SMC sampler...\n",
      "INFO:pymc.smc.sampling:Initializing SMC sampler...\n",
      "Sampling 4 chains in 4 jobs\n",
      "INFO:pymc.smc.sampling:Sampling 4 chains in 4 jobs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46bd985e23b4b26864a02f24a7cfe22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "INFO:pymc.stats.convergence:The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "ERROR:pymc.stats.convergence:The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "SMC sampling completed in 25.24 seconds\n",
      "INFO:pymc:SMC sampling completed in 25.24 seconds\n",
      "Starting posterior predictive sampling...\n",
      "INFO:pymc:Starting posterior predictive sampling...\n",
      "Sampling: [observed_counts]\n",
      "INFO:pymc.sampling.forward:Sampling: [observed_counts]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e79896747847fb8d03c7b1a6cd4542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Posterior predictive sampling completed in 25.48 seconds\n",
      "INFO:pymc:Posterior predictive sampling completed in 25.48 seconds\n",
      "Raw trace posterior structure:\n",
      "INFO:pymc:Raw trace posterior structure:\n",
      "<xarray.Dataset>\n",
      "Dimensions:              (chain: 4, draw: 500, user_prefs_dim_0: 50,\n",
      "                          user_prefs_dim_1: 3, song_features_dim_0: 50,\n",
      "                          song_features_dim_1: 3)\n",
      "Coordinates:\n",
      "  * chain                (chain) int64 0 1 2 3\n",
      "  * draw                 (draw) int64 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n",
      "  * user_prefs_dim_0     (user_prefs_dim_0) int64 0 1 2 3 4 5 ... 45 46 47 48 49\n",
      "  * user_prefs_dim_1     (user_prefs_dim_1) int64 0 1 2\n",
      "  * song_features_dim_0  (song_features_dim_0) int64 0 1 2 3 4 ... 46 47 48 49\n",
      "  * song_features_dim_1  (song_features_dim_1) int64 0 1 2\n",
      "Data variables:\n",
      "    user_prefs           (chain, draw, user_prefs_dim_0, user_prefs_dim_1) float64 ...\n",
      "    song_features        (chain, draw, song_features_dim_0, song_features_dim_1) float64 ...\n",
      "Attributes:\n",
      "    created_at:                 2025-09-22T15:36:23.764289+00:00\n",
      "    arviz_version:              0.21.0\n",
      "    inference_library:          pymc\n",
      "    inference_library_version:  5.23.0\n",
      "INFO:pymc:<xarray.Dataset>\n",
      "Dimensions:              (chain: 4, draw: 500, user_prefs_dim_0: 50,\n",
      "                          user_prefs_dim_1: 3, song_features_dim_0: 50,\n",
      "                          song_features_dim_1: 3)\n",
      "Coordinates:\n",
      "  * chain                (chain) int64 0 1 2 3\n",
      "  * draw                 (draw) int64 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\n",
      "  * user_prefs_dim_0     (user_prefs_dim_0) int64 0 1 2 3 4 5 ... 45 46 47 48 49\n",
      "  * user_prefs_dim_1     (user_prefs_dim_1) int64 0 1 2\n",
      "  * song_features_dim_0  (song_features_dim_0) int64 0 1 2 3 4 ... 46 47 48 49\n",
      "  * song_features_dim_1  (song_features_dim_1) int64 0 1 2\n",
      "Data variables:\n",
      "    user_prefs           (chain, draw, user_prefs_dim_0, user_prefs_dim_1) float64 ...\n",
      "    song_features        (chain, draw, song_features_dim_0, song_features_dim_1) float64 ...\n",
      "Attributes:\n",
      "    created_at:                 2025-09-22T15:36:23.764289+00:00\n",
      "    arviz_version:              0.21.0\n",
      "    inference_library:          pymc\n",
      "    inference_library_version:  5.23.0\n",
      "Transposing user_prefs_samples from (50, 3, 2000) to (2000, 50, 3)\n",
      "INFO:pymc:Transposing user_prefs_samples from (50, 3, 2000) to (2000, 50, 3)\n",
      "Transposing song_features_samples from (50, 3, 2000) to (2000, 50, 3)\n",
      "INFO:pymc:Transposing song_features_samples from (50, 3, 2000) to (2000, 50, 3)\n",
      "user_prefs_samples shape: (2000, 50, 3), expected: (2000, 50, 3)\n",
      "INFO:pymc:user_prefs_samples shape: (2000, 50, 3), expected: (2000, 50, 3)\n",
      "song_features_samples shape: (2000, 50, 3), expected: (2000, 50, 3)\n",
      "INFO:pymc:song_features_samples shape: (2000, 50, 3), expected: (2000, 50, 3)\n",
      "Saving traces to NetCDF...\n",
      "INFO:pymc:Saving traces to NetCDF...\n",
      "Traces saved successfully\n",
      "INFO:pymc:Traces saved successfully\n",
      "Convergence diagnostics:\n",
      "INFO:pymc:Convergence diagnostics:\n",
      "Generating t-SNE visualization...\n",
      "INFO:pymc:Generating t-SNE visualization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mean    sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
      "user_prefs[0, 0]      0.29  0.71   -0.36     1.45       0.35     0.17   \n",
      "user_prefs[0, 1]     -0.91  1.24   -2.93     0.47       0.62     0.31   \n",
      "user_prefs[0, 2]     -0.01  0.48   -0.69     0.72       0.24     0.12   \n",
      "user_prefs[1, 0]     -0.82  0.72   -1.46     0.39       0.35     0.18   \n",
      "user_prefs[1, 1]      0.62  1.37   -1.77     1.71       0.68     0.39   \n",
      "...                    ...   ...     ...      ...        ...      ...   \n",
      "song_features[48, 1] -0.53  0.88   -1.21     1.00       0.44     0.25   \n",
      "song_features[48, 2]  1.04  0.75   -0.26     1.79       0.37     0.20   \n",
      "song_features[49, 0]  0.19  0.12    0.06     0.43       0.06     0.03   \n",
      "song_features[49, 1]  0.28  0.53   -0.46     1.08       0.26     0.13   \n",
      "song_features[49, 2]  0.36  0.95   -1.24     1.31       0.47     0.25   \n",
      "\n",
      "                      ess_bulk  ess_tail  r_hat  \n",
      "user_prefs[0, 0]          4.79     31.91   2.61  \n",
      "user_prefs[0, 1]          4.66     30.75   2.84  \n",
      "user_prefs[0, 2]          4.66     28.84   2.85  \n",
      "user_prefs[1, 0]          4.72     30.12   2.72  \n",
      "user_prefs[1, 1]          4.75     29.46   2.67  \n",
      "...                        ...       ...    ...  \n",
      "song_features[48, 1]      4.72     32.04   2.74  \n",
      "song_features[48, 2]      4.67     29.99   2.84  \n",
      "song_features[49, 0]      5.15     31.52   2.22  \n",
      "song_features[49, 1]      4.66     32.33   2.84  \n",
      "song_features[49, 2]      4.66     29.00   2.85  \n",
      "\n",
      "[300 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualization saved as music_viz.html\n",
      "INFO:pymc:Visualization saved as music_viz.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.manifold import TSNE\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from scipy.sparse import csr_matrix\n",
    "import logging\n",
    "import time\n",
    "import warnings\n",
    "import pytensor\n",
    "import sys\n",
    "import xarray as xr\n",
    "import os \n",
    "\n",
    "# Disable rich console output in Jupyter to prevent recursion\n",
    "try:\n",
    "    from rich.console import Console\n",
    "    Console()._live = None  # Disable rich's live display\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Redirect warnings to avoid recursive output\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pytensor')\n",
    "\n",
    "# Set PyTensor BLAS config to avoid KeyError\n",
    "pytensor.config.blas__ldflags = ''\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "\n",
    "# 1. Load and Preprocess Data (Download and Unzip)\n",
    "logger.info(\"Loading and unzipping dataset...\")\n",
    "start_time = time.time()\n",
    "if os.path.exists('train_triplets.txt'):\n",
    "    df = pd.read_csv('train_triplets.txt', sep='\\t', names=['user', 'song', 'count'])\n",
    "else:\n",
    "    url = \"http://labrosa.ee.columbia.edu/~dpwe/tmp/train_triplets.txt.zip\"\n",
    "    response = requests.get(url)\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        with z.open('train_triplets.txt') as f:\n",
    "            df = pd.read_csv(f, sep='\\t', names=['user', 'song', 'count'])\n",
    "logger.info(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Subsample for efficiency\n",
    "n_users, n_songs = 50, 50  # Reduced to minimize computation\n",
    "logger.info(f\"Subsampling {n_users} users and {n_songs} songs...\")\n",
    "top_users = df['user'].value_counts().head(n_users + 50).index\n",
    "top_songs = df['song'].value_counts().head(n_songs + 50).index\n",
    "df = df[df['user'].isin(top_users) & df['song'].isin(top_songs)]\n",
    "\n",
    "# Create user-song matrix and reindex\n",
    "play_counts = df.pivot_table(index='user', columns='song', values='count', fill_value=0)\n",
    "all_users = top_users[:n_users]\n",
    "all_songs = top_songs[:n_songs]\n",
    "play_counts = play_counts.reindex(index=all_users, columns=all_songs, fill_value=0)\n",
    "play_counts = csr_matrix(play_counts.values)\n",
    "user_ids = pd.Categorical(all_users).categories\n",
    "song_ids = pd.Categorical(all_songs).categories\n",
    "assert play_counts.shape == (n_users, n_songs), f\"Play count matrix shape {play_counts.shape}, expected {(n_users, n_songs)}\"\n",
    "assert len(user_ids) == n_users, f\"Got {len(user_ids)} users, expected {n_users}\"\n",
    "assert len(song_ids) == n_songs, f\"Got {len(song_ids)} songs, expected {n_songs}\"\n",
    "logger.info(f\"Play counts matrix shape: {play_counts.shape}\")\n",
    "\n",
    "# Normalize counts to [0, 1]\n",
    "play_counts_dense = play_counts.toarray()\n",
    "play_counts_norm = play_counts_dense / (play_counts_dense.max() + 1e-8)\n",
    "\n",
    "# 2. Bayesian ABC Model (Matrix Factorization)\n",
    "n_latent = 3\n",
    "logger.info(\"Starting SMC sampling...\")\n",
    "with pm.Model() as abc_model:\n",
    "    user_prefs = pm.Normal('user_prefs', mu=0, sigma=1, shape=(n_users, n_latent))\n",
    "    song_features = pm.Normal('song_features', mu=0, sigma=1, shape=(n_songs, n_latent))\n",
    "    mu = pm.math.sigmoid(pm.math.dot(user_prefs, song_features.T))\n",
    "    pm.Normal('observed_counts', mu=mu, sigma=0.1, observed=play_counts_norm)\n",
    "    start_time = time.time()\n",
    "    trace = pm.sample_smc(500, cores=4, return_inferencedata=True, progressbar=True)\n",
    "    logger.info(f\"SMC sampling completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# 3. Posterior Predictive Sampling\n",
    "logger.info(\"Starting posterior predictive sampling...\")\n",
    "with abc_model:\n",
    "    pred_trace = pm.sample_posterior_predictive(trace, var_names=['observed_counts'])\n",
    "logger.info(f\"Posterior predictive sampling completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# 4. Extract Posterior and Predictive Samples\n",
    "logger.info(\"Raw trace posterior structure:\")\n",
    "logger.info(f\"{trace.posterior}\")\n",
    "posterior = az.extract(trace, combined=True)  # Combine chains and draws\n",
    "user_prefs_samples = np.asarray(posterior['user_prefs'].values, dtype=np.float64)  # Shape: (draws × chains, n_users, n_latent)\n",
    "song_features_samples = np.asarray(posterior['song_features'].values, dtype=np.float64)  # Shape: (draws × chains, n_songs, n_latent)\n",
    "\n",
    "# Check and correct dimension order\n",
    "expected_draws = 500 * 4  # 500 draws × 4 chains\n",
    "if user_prefs_samples.shape != (expected_draws, n_users, n_latent):\n",
    "    logger.info(f\"Transposing user_prefs_samples from {user_prefs_samples.shape} to ({expected_draws}, {n_users}, {n_latent})\")\n",
    "    user_prefs_samples = user_prefs_samples.transpose(2, 0, 1)\n",
    "if song_features_samples.shape != (expected_draws, n_songs, n_latent):\n",
    "    logger.info(f\"Transposing song_features_samples from {song_features_samples.shape} to ({expected_draws}, {n_songs}, {n_latent})\")\n",
    "    song_features_samples = song_features_samples.transpose(2, 0, 1)\n",
    "\n",
    "logger.info(f\"user_prefs_samples shape: {user_prefs_samples.shape}, expected: ({expected_draws}, {n_users}, {n_latent})\")\n",
    "logger.info(f\"song_features_samples shape: {song_features_samples.shape}, expected: ({expected_draws}, {n_songs}, {n_latent})\")\n",
    "assert user_prefs_samples.shape == (expected_draws, n_users, n_latent), f\"Unexpected shape for user_prefs_samples: {user_prefs_samples.shape}\"\n",
    "assert song_features_samples.shape == (expected_draws, n_songs, n_latent), f\"Unexpected shape for song_features_samples: {song_features_samples.shape}\"\n",
    "pred_counts = pred_trace.posterior_predictive['observed_counts'].mean(axis=(0, 1)) * play_counts_dense.max()\n",
    "\n",
    "# Save traces as simplified InferenceData\n",
    "logger.info(\"Saving traces to NetCDF...\")\n",
    "try:\n",
    "    # Create separate Datasets for user_prefs and song_features\n",
    "    user_prefs_ds = xr.Dataset(\n",
    "        {\n",
    "            'user_prefs': (['draw', 'user', 'latent'], user_prefs_samples)\n",
    "        },\n",
    "        coords={\n",
    "            'draw': np.arange(user_prefs_samples.shape[0]),\n",
    "            'user': np.arange(n_users),\n",
    "            'latent': np.arange(n_latent)\n",
    "        }\n",
    "    )\n",
    "    song_features_ds = xr.Dataset(\n",
    "        {\n",
    "            'song_features': (['draw', 'song', 'latent'], song_features_samples)\n",
    "        },\n",
    "        coords={\n",
    "            'draw': np.arange(song_features_samples.shape[0]),\n",
    "            'song': np.arange(n_songs),\n",
    "            'latent': np.arange(n_latent)\n",
    "        }\n",
    "    )\n",
    "    # Merge Datasets\n",
    "    posterior_ds = xr.merge([user_prefs_ds, song_features_ds])\n",
    "    # Save posterior trace\n",
    "    posterior_ds.to_netcdf('music_trace.nc')\n",
    "    # Save posterior predictive trace\n",
    "    az.to_netcdf(pred_trace, 'music_pred_trace.nc')\n",
    "    logger.info(\"Traces saved successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to save traces: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Check convergence\n",
    "logger.info(\"Convergence diagnostics:\")\n",
    "print(az.summary(trace, var_names=['user_prefs', 'song_features'], round_to=2))\n",
    "\n",
    "# 5. Interactive Visualization (t-SNE with ABC Animation)\n",
    "logger.info(\"Generating t-SNE visualization...\")\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "user_prefs_mean = user_prefs_samples.mean(axis=0)\n",
    "user_tsne = tsne.fit_transform(user_prefs_mean)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('User Preference Posterior', 't-SNE User Embeddings'))\n",
    "\n",
    "fig.add_trace(go.Histogram(x=user_prefs_samples[:, 0, 0], name='User 0: Latent Dim 1', \n",
    "                           marker_color='blue', opacity=0.6), rows=row, cols=col)\n",
    "fig.add_trace(go.Scatter(x=user_tsne[:, 0], y=user_tsne[:, 1], mode='markers',\n",
    "                         marker=dict(size=5, color='blue'), name='Users'), row=1, col=2)\n",
    "\n",
    "# Animation for posterior draws (3 frames)\n",
    "n_frames = 3\n",
    "draws = np.linspace(0, user_prefs_samples.shape[0] - 1, n_frames, dtype=int)\n",
    "frames = []\n",
    "for i, draw in enumerate(draws):\n",
    "    tsne_frame = TSNE(n_components=2, random_state=42).fit_transform(user_prefs_samples[draw])\n",
    "    frame = go.Frame(\n",
    "        data=[go.Scatter(x=tsne_frame[:, 0], y=tsne_frame[:, 1], mode='markers',\n",
    "                         marker=dict(size=5, color='blue'), name='Users')],\n",
    "        name=f'Draw {i}'\n",
    "    )\n",
    "    frames.append(frame)\n",
    "\n",
    "fig.update(frames=frames)\n",
    "fig.update_layout(\n",
    "    title='Bayesian ABC for Music Recommendations',\n",
    "    xaxis2_title='t-SNE Dim 1', yaxis2_title='t-SNE Dim 2',\n",
    "    updatemenus=[{\n",
    "        'buttons': [\n",
    "            {'method': 'animate', 'label': 'Play', 'args': [None, {'frame': {'duration': 500, 'redraw': True}, 'fromcurrent': True}]},\n",
    "            {'method': 'animate', 'label': 'Pause', 'args': [[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate'}]}\n",
    "        ],\n",
    "        'direction': 'left', 'pad': {'r': 10, 't': 87}, 'showactive': True, 'type': 'buttons'\n",
    "    }],\n",
    "    showlegend=True\n",
    ")\n",
    "fig.write_html('music_viz.html')\n",
    "logger.info(\"Visualization saved as music_viz.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
